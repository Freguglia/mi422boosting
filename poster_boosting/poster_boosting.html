<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Victor Freguglia; Leonarcho Uchoa Pedreira" />
  <title>Algoritmos de Boosting</title>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="drposter_files//drposter.css" />
    <link rel="stylesheet" href="custom.css"/>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div class="slides ">

<header>
    <h1 class="title">Algoritmos de Boosting</h1>
    <h2 class="author">Victor Freguglia; Leonarcho Uchoa Pedreira</h2>
</header>

<div id="section" class="section level1 col-2">
<h1></h1>
<div id="o-conceito-de-boosting" class="section level2">
<h2>O Conceito de Boosting</h2>
<ul>
<li>Combinar um grande número de preditores com baixo poder de predição para compor um bom preditor.</li>
<li>Diferentemente de outros métodos similares, onde os preditores fracos combinados são criados de maneira independente, no Boosting os preditores de maneira a melhorar o desempenho em regiões com altas taxas de erro.</li>
<li>Se pensarmos que cada preditor tem um “voto” na decisão final, o método nos fornece um comitê em que aqueles que tem grande convicção têm mais poder na decisão.</li>
</ul>
</div>
<div id="visualizacao" class="section level2">
<h2>Visualização</h2>
<div id="primeiras-iteracoes" class="section level3">
<h3>Primeiras iterações</h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-1-1.png" width="768" /></p>
</div>
<div id="section-1" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-2-1.png" width="768" /></p>
</div>
<div id="section-2" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-3-1.png" width="768" /></p>
</div>
<div id="section-3" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
</div>
<div id="section-4" class="section level3 fullwidth">
<h3>…</h3>
</div>
<div id="section-5" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-5-1.png" width="768" /></p>
</div>
<div id="section-6" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-6-1.png" width="768" /></p>
</div>
</div>
</div>
<div id="section-7" class="section level1 col-2">
<h1></h1>
<div id="gradient-boosting" class="section level2">
<h2>Gradient Boosting</h2>
<div id="algoritmo" class="section level3">
<h3>Algoritmo</h3>
<ul>
<li><span class="math inline">\((y_i, x_i), i = 1,\dots, N\)</span>;</li>
<li><span class="math inline">\(L(\cdot, \cdot)\)</span> - função perda;</li>
<li>Inicie com o preditor constante <span class="math inline">\(H_0 = \arg \min_c \sum_{i = 1}^N L(y_i,c)\)</span>;</li>
<li>Para <span class="math inline">\(m = 1, \dots, M\)</span> faça:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Calcular <span class="math display">\[r_{im} = -\left[ \frac{\partial L(y_i, G(x_i))}{\partial }\right]_{G = H_{m-1}};\]</span></li>
<li>Ajustar um novo preditor fraco <span class="math inline">\(h_m\)</span> aos resíduos <span class="math inline">\((r_{im},x_i);\)</span></li>
<li>Calcule o multiplicador <span class="math inline">\(\gamma_{m}\)</span> como <span class="math display">\[\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, H_{m-1}(x_i) + \gamma h_m(x_i)\right);\]</span></li>
<li>Atualize o modelo: <span class="math display">\[\displaystyle H_{m}(x)=H_{m-1}(x)+\gamma_m h_m(x);\]</span></li>
</ol>
<ul>
<li>Defina o classificador final como <span class="math display">\[G(x) = H_M(x).\]</span></li>
</ul>
</div>
<div id="vantagens" class="section level3">
<h3>Vantagens</h3>
<ul>
<li>Garante, ao menos na amostra de treino, um desempenho melhor a cada passo.</li>
<li>Funciona para Regressão e classificação com adaptação na função perda.</li>
<li>Não requer nenhum tipo de pré-processamento.</li>
</ul>
</div>
<div id="desvantagens" class="section level3">
<h3>Desvantagens</h3>
<ul>
<li>Alto custo computacional;</li>
<li>É possível (embora pouco provável) ocorrer overfitting;</li>
<li>Não interpretável;</li>
</ul>
</div>
<div id="tuning" class="section level3 fullwidth">
<h3>Tuning</h3>
<p>Algumas variações do Gradient Boosting incluem:</p>
<ul>
<li>Sortear um subconjunto de tamanho <span class="math inline">\(N&#39; &lt; N\)</span> da amostra de treino para o ajuste do preditor a cada passo.</li>
<li>Encolher os preditores por algum valor <span class="math inline">\(\alpha &lt; 1\)</span>, isto é, substituir o passo de atualização por <span class="math display">\[\displaystyle H_{m}(x)=H_{m-1}(x)+\alpha \gamma_m h_m(x).\]</span></li>
<li>Utilizar diferentes números de nós quando o preditor fraco utlizado é uma Árvore de Regressão e Classificação.</li>
</ul>
</div>
<div id="implementacao" class="section level3 fullwidth">
<h3>Implementação</h3>
<p>Algumas das principais implementações do algoritmo de Gradient Boosting em <code>R</code> são:</p>
<ul>
<li>Pacote <code>gbm</code>: Generalized Boosted Regression Models;</li>
<li>Pacote <code>xgboost</code>: Extreme Gradient Boosting;</li>
<li>Plataforma <code>h2o</code>: www.h2o.ai</li>
</ul>
</div>
</div>
</div>
<div id="section-8" class="section level1 col-2">
<h1></h1>
<div id="uma-aplicacao" class="section level2">
<h2>Uma aplicação</h2>
<div id="conjunto-de-dados-mnist" class="section level3">
<h3>Conjunto de dados MNIST</h3>
<div class="figure">
<img src="mnist.jpg" />

</div>
</div>
<div id="resultados" class="section level3">
<h3>Resultados</h3>
<ul>
<li>60000 Imagens 28x28 pixels de dígitos escritos a mão.</li>
<li>Classificação dos dígitos (10 categorias) utilizando os valores dos 784 pixels como covariáveis.</li>
<li>Acurácia de 97.33% no conjunto de teste da competição no Kaggle, utilizando Gradient Boosting, com taxa de aprendizado <span class="math inline">\(\alpha = 0.08\)</span>, árvores com 7 nós nos classificadores e 600 passos de Boosting, sem nenhum tipo de pré-processamento.</li>
<li>Implementação em <code>R</code> utilizando o framework <code>h2o</code> e resultados no Kaggle disponíveis no QR-code.</li>
</ul>
<p><img src="frame.png" class="qrcode"/></p>
</div>
</div>
</div>
<div id="section-9" class="section level1 col-2">
<h1></h1>
<div id="conclusao" class="section level2">
<h2>Conclusão</h2>
<ul>
<li>Boosting produz classificadores muito eficiêntes;</li>
<li>Apesar das variações incluirem diversos parâmetros, a variação deles, exceto para casos específicos, não tem grande efeito na qualidade final das predições; Por outro lado, podem reduzir o custo computacional ou alterar a quantidade necessária de passos até produzir um bom preditor;</li>
<li>A forma com que o algoritmo é construído causa a impressão de que o overfitting deve ocorrer, mas a quantidade necessária de passos para que ele de fato ocorra é muito grande. A visualização exemplifica como os passos tendem a modificar cada vez menos o modelo.</li>
<li>Por se tratar de modificações sequênciais de um único preditor, não é necessário guardar um grande número de modelos simutaneamente. Por outro lado, isso torna a paralelização mais complicada.</li>
</ul>
</div>
<div id="referencias" class="section level2">
<h2>Referências</h2>
<ul>
<li>Friedman, J. H. (2001), ‘Greedy function approximation: a gradient boosting machine’, Annals of statistics pp. 1189–1232.</li>
<li>Friedman, J., Hastie, T. &amp; Tibshirani, R. (2001), The elements of statistical learning, Vol. 1, Springer series in statistics New York, NY, USA:.</li>
<li>Schapire, R. E. (1990), ‘The strenght of weak learnability’, Machine Learning 5, 197–227.</li>
</ul>
</div>
</div>
</div>

  </body>
</html>
