<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Victor Freguglia; Leonarcho Uchoa Pedreira" />
  <title>Boosting</title>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="stylesheet" href="drposter_files//drposter.css" />
    <link rel="stylesheet" href="custom.css"/>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div class="slides ">

<header>
    <h1 class="title">Boosting</h1>
    <h2 class="author">Victor Freguglia; Leonarcho Uchoa Pedreira</h2>
</header>

<div id="section" class="section level1 col-2">
<h1></h1>
<div id="o-conceito-de-boosting" class="section level2">
<h2>O Conceito de Boosting</h2>
<ul>
<li>Combinar um grande número de preditores simples para compor um bom preditor.</li>
<li>Os preditores (árvores de decisão) são ajustados sequencialmente de maneira a melhorar o desempenho do anterior, ao compreender melhor as regiões com altas taxas de erro.</li>
<li>Se pensarmos que cada árvore tem um “voto” na decisão final, o método nos fornece um comitê em que a decisão final é a ponderação de todos os votos. Aqueles que têm grande convicção têm mais poder na decisão.</li>
</ul>
</div>
<div id="visualizacao" class="section level2">
<h2>Visualização</h2>
<div id="primeiras-iteracoes" class="section level3">
<h3>Primeiras iterações</h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="residuos" class="section level3">
<h3>Resíduos</h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="section-1" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="section-2" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="section-3" class="section level3 fullwidth">
<h3>…</h3>
</div>
<div id="section-4" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="section-5" class="section level3">
<h3></h3>
<p><img src="poster_boosting_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="section-6" class="section level1 col-2">
<h1></h1>
<div id="gradient-boosting" class="section level2">
<h2>Gradient Boosting</h2>
<p>Direção fornecida da solução pelo método do Gradiente e sua conexão com os resíduos fornecem a intuição e a engrenagem por detrás do método.</p>
<div id="algoritmo" class="section level3">
<h3>Algoritmo</h3>
<ul>
<li><span class="math inline">\((y_i, x_i), i = 1,\dots, N\)</span>;</li>
<li><span class="math inline">\(L(\cdot, \cdot)\)</span> - função perda;</li>
<li>Inicie com o preditor constante <span class="math inline">\(H_0 = \arg \min_c \sum_{i = 1}^N L(y_i,c)\)</span>;</li>
<li>Para <span class="math inline">\(m = 1, \dots, M\)</span> faça:</li>
</ul>
<ol style="list-style-type: decimal">
<li>Calcular os pseudo-resíduos <span class="math display">\[r_{im} = -\left[ \frac{\partial L(y_i, G(x_i))}{\partial G(x_i) }\right]_{G(x_i) = H_{m-1}(x_i)};\]</span></li>
<li>Ajustar uma nova árvore <span class="math inline">\(h_m\)</span> aos resíduos <span class="math inline">\((r_{im},x_i);\)</span></li>
<li>Calcule o peso do voto <span class="math inline">\(\gamma_{m}\)</span> como <span class="math display">\[\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, H_{m-1}(x_i) + \gamma h_m(x_i)\right);\]</span></li>
<li>Atualize o modelo: <span class="math display">\[\displaystyle H_{m}(x)=H_{m-1}(x)+\gamma_m h_m(x);\]</span></li>
</ol>
<ul>
<li>Defina o preditor final como <span class="math display">\[G(x) = H_M(x).\]</span></li>
</ul>
</div>
<div id="vantagens" class="section level3">
<h3>Vantagens</h3>
<ul>
<li>Alto poder de preditivo (ganhador de vários concursos recentes)</li>
<li>Otimização direta da função perda (regressão, classificação, regressão quantílica, Modelo de Cox, etc…).</li>
<li>Não requer nenhum tipo de pré-processamento e pode ser paralelizado.</li>
</ul>
</div>
<div id="desvantagens" class="section level3">
<h3>Desvantagens</h3>
<ul>
<li>Alto custo computacional;</li>
<li>Pode ser muito sensível a dados muito ruidosos;</li>
<li>Requer refinamento cuidadoso;</li>
</ul>
</div>
<div id="refinamento" class="section level3 fullwidth">
<h3>Refinamento</h3>
<ul>
<li>Subamostragem: Sortear um subconjunto de tamanho <span class="math inline">\(N&#39; &lt; N\)</span> da amostra de treino para o ajuste do preditor a cada passo.</li>
<li>Penalização: Encolher os preditores por algum valor <span class="math inline">\(\alpha &lt; 1\)</span>, isto é, substituir o passo de atualização por <span class="math display">\[\displaystyle H_{m}(x)=H_{m-1}(x)+\alpha \gamma_m h_m(x).\]</span></li>
<li>Número de nós terminais: Utilizar diferentes números de nós terminais para as árvores.</li>
</ul>
</div>
<div id="implementacao" class="section level3 fullwidth">
<h3>Implementação</h3>
<ul>
<li>Pacote <code>gbm</code>: Generalized Boosted Regression Models;</li>
<li>Pacote <code>xgboost</code>: Extreme Gradient Boosting;</li>
<li>Plataforma <code>h2o</code>: www.h2o.ai;</li>
<li>Pacote <code>LightGBM</code>: Implementação da Microsoft para gbm;</li>
</ul>
</div>
</div>
</div>
<div id="section-7" class="section level1 col-2">
<h1></h1>
<div id="uma-aplicacao" class="section level2">
<h2>Uma aplicação</h2>
<div id="conjunto-de-dados-mnist" class="section level3">
<h3>Conjunto de dados MNIST</h3>
<div class="figure">
<img src="mnist.jpg" />

</div>
<ul>
<li>60000 Imagens 28x28 pixels de dígitos escritos a mão. Classificação dos dígitos.</li>
</ul>
</div>
<div id="resultados" class="section level3">
<h3>Resultados</h3>
<ul>
<li>Acurácia de 97.33% no conjunto de teste da competição no Kaggle, utilizando Gradient Boosting, com taxa de aprendizado <span class="math inline">\(\alpha = 0.08\)</span>, árvores com 7 nós nos classificadores e 600 passos de Boosting, <strong>sem nenhum tipo de pré-processamento</strong>.</li>
<li>Implementação em <code>R</code> utilizando o framework <code>h2o</code> e resultados no Kaggle disponíveis no QR-code.</li>
</ul>
<p><img src="frame.png" class="qrcode"/></p>
</div>
</div>
</div>
<div id="section-8" class="section level1 col-2">
<h1></h1>
<div id="conclusao" class="section level2">
<h2>Conclusão</h2>
<ul>
<li>Boosting produz preditores muito eficazes;</li>
<li>Extretamente versátil;</li>
<li>Apesar de incluir diversos parâmetros de refinamento, exceto em casos específicos, muitos não tem grande efeito na qualidade final das predições; Por outro lado, podem drasticamente reduzir o custo computacional ou alterar a quantidade necessária de passos até produzir um bom preditor;</li>
<li>A forma com que o algoritmo é construído causa a impressão de que o overfitting deve ocorrer, mas a quantidade necessária de passos para que ele de fato ocorra é muito grande.</li>
</ul>
</div>
<div id="referencias" class="section level2">
<h2>Referências</h2>
<ul>
<li>Friedman, J. H. (2001), ‘Greedy function approximation: a gradient boosting machine’, Annals of statistics pp. 1189–1232.</li>
<li>Friedman, J., Hastie, T. &amp; Tibshirani, R. (2001), The elements of statistical learning, Vol. 1, Springer series in statistics New York, NY, USA:.</li>
</ul>
</div>
</div>
</div>

  </body>
</html>
