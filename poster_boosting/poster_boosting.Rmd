---
title: "Algoritmos de Boosting"
author: "Victor Freguglia; Leonarcho Uchoa Pedreira"
output:
  drposter::drposter_poster:
    fill_page: FALSE
    self_contained: FALSE
references:
- id: ESL
  author: Hastie, Tibishirani
  title: 'Elements of Statistical Learning'
  type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = TRUE, message = FALSE,
                      fig.width = 8)  # Hide code by default. Can override chunk-by-chunk
library(tidyverse)
library(rpart)
library(caret)
```


# {.col-2}

## O Conceito de Boosting

* Combinar um grande número de preditores com baixo poder de predição para compor um bom preditor.
* Diferentemente de outros métodos similares, onde os preditores fracos combinados são criados de maneira independente, no Boosting os preditores de maneira a melhorar o desempenho em regiões com altas taxas de erro. 
* Se pensarmos que cada preditor tem um "voto" na decisão final, o método nos fornece um comitê em que aqueles que tem grande convicção têm mais poder na decisão.

## Visualização

###
```{r}
set.seed(5)
df <- data.frame(x = runif(100, 0, 5)) %>% mutate(y = rnorm(100, mean = 1.8*x))
m1 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m1)), color = "red") + theme_bw() + ggtitle("H1") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df2 <- df; df2$y <- residuals(m1)
m2 <- train(y~x,df2, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df2, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m2)), color = "red") + theme_bw() + ggtitle("Resíduos H1 e h2") + theme(plot.title = element_text(size=22, hjust = 0.5))
``` 

###
```{r}
m3 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(2),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m3)), color = "red") + theme_bw() + ggtitle("H2") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df4 <- df; df4$y <- residuals(m3)
m4 <-  train(y~x,df4, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df4, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m4)), color = "red") + theme_bw() + ggtitle("Resíduos H2 e h3") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
m5 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(20),
                                   max_depth = c(1),
                                   eta = c(.8),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m5)), color = "red") + theme_bw() + ggtitle("H20") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df5 <- df; df5$y <- residuals(m5)
m6 <-  train(y~x,df5, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df5, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m6)), color = "red") + theme_bw() + ggtitle("Resíduos H20 e h21") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

# {.col-2}

## Gradient Boosting

### Algoritmo
* $(y_i, x_i), i = 1,\dots, N$;
* $L(\cdot, \cdot)$ - função perda;
* Inicie com o preditor constante $H_0 = \arg \min_c \sum_{i = 1}^N L(y_i,c)$;
* Para $m = 1, \dots, M$ faça:
  1. Calcular
    $$r_{im} = -\left[ \frac{\partial L(y_i, G(x_i))}{\partial }\right]_{G = H_{m-1}};$$
  2. Ajustar um novo preditor fraco $h_m$ aos resíduos $(r_{im},x_i);$
  3. Calcule o multiplicador $\gamma_{m}$ como 
  $$\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, H_{m-1}(x_i) + \gamma h_m(x_i)\right);$$
  4. Atualize o modelo:
$$\displaystyle H_{m}(x)=H_{m-1}(x)+\gamma_m h_m(x);$$
* Defina o classificador final como 
  $$G(x) = H_M(x).$$

### Vantagens
* Garante, ao menos na amostra de treino, um desempenho melhor a cada passo.
* Funciona para Regressão e classificação com adaptação na função perda.

### Desvantagens
* Alto custo computacional;
* Iterações dependentes dificultam a paralelização;
* Não interpretável;

### Tuning {.fullwidth}
Algumas variações do Gradient Boosting incluem:

 * Sortear um subconjunto de tamanho $N' < N$ da amostra de treino para o ajuste do preditor a cada passo.
 * Encolher os preditores por algum valor $\alpha < 1$, isto é, substituir o passo de atualização por 
 $$\displaystyle H_{m}(x)=H_{m-1}(x)+\alpha \gamma_m h_m(x).$$
 * Utilizar diferentes números de nós quando o preditor fraco utlizado é uma Árvore de Regressão e Classificação.



# {.col-2}

## Uma aplicação

### Conjunto de dados MNIST

![](mnist.jpg)


### Resultados


* Imagens 32x32 pixels
* Feito com h2o.
* Deve chegar a 97,12%

<img src="frame.png" class="qrcode"/>

# {.col-2}

## Referências
