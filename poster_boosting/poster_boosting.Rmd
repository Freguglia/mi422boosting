---
title: "Algoritmos de Boosting"
author: "Victor Freguglia; Leonarcho Uchoa Pedreira"
output:
  drposter::drposter_poster:
    fill_page: FALSE
    self_contained: FALSE
references:
- id: ESL
  author: Hastie, Tibishirani
  title: 'Elements of Statistical Learning'
  type: book
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = TRUE, message = FALSE,
                      fig.width = 8)  # Hide code by default. Can override chunk-by-chunk
library(tidyverse)
library(rpart)
library(caret)
```


# {.col-2}

## O Conceito de Boosting

* Combinar um grande número de preditores com baixo poder de predição para compor um bom preditor.
* Diferentemente de outros métodos similares, onde os preditores fracos combinados são criados de maneira independente, no Boosting os preditores de maneira a melhorar o desempenho em regiões com altas taxas de erro. 
* Se pensarmos que cada preditor tem um "voto" na decisão final, o método nos fornece um comitê em que aqueles que tem grande convicção têm mais poder na decisão.

## Visualização

### Primeiras iterações
```{r}
set.seed(5)
df <- data.frame(x = runif(100, 0, 5)) %>% mutate(y = rnorm(100, mean = 1.8*x))
m1 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m1)), color = "red") + theme_bw() + ggtitle("H1") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df2 <- df; df2$y <- residuals(m1)
m2 <- train(y~x,df2, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df2, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m2)), color = "red") + theme_bw() + ggtitle("Resíduos H1 e h2") + theme(plot.title = element_text(size=22, hjust = 0.5))
``` 

###
```{r}
m3 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(2),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m3)), color = "red") + theme_bw() + ggtitle("H2") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df4 <- df; df4$y <- residuals(m3)
m4 <-  train(y~x,df4, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df4, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m4)), color = "red") + theme_bw() + ggtitle("Resíduos H2 e h3") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

### ... {.fullwidth}

###
```{r}
m5 <- train(y~x,df, method = "xgbTree", tuneGrid = data.frame(nrounds = c(20),
                                   max_depth = c(1),
                                   eta = c(.8),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df, aes(x = x, y = y)) + geom_point(color = "blue", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m5)), color = "red") + theme_bw() + ggtitle("H20") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

###
```{r}
df5 <- df; df5$y <- residuals(m5)
m6 <-  train(y~x,df5, method = "xgbTree", tuneGrid = data.frame(nrounds = c(1),
                                   max_depth = c(1),
                                   eta = c(1),
                                   gamma = 0,
                                   colsample_bytree = 1,
                                   subsample = 1,
                                   min_child_weight = 1))
ggplot(df5, aes(x = x, y = y)) + geom_point(color = "orange", size = 3, alpha = 0.5) + geom_line(aes(x = x, y = predict(m6)), color = "red") + theme_bw() + ggtitle("Resíduos H20 e h21") + theme(plot.title = element_text(size=22, hjust = 0.5))
```

# {.col-2}

## Gradient Boosting

### Algoritmo
* $(y_i, x_i), i = 1,\dots, N$;
* $L(\cdot, \cdot)$ - função perda;
* Inicie com o preditor constante $H_0 = \arg \min_c \sum_{i = 1}^N L(y_i,c)$;
* Para $m = 1, \dots, M$ faça:
  1. Calcular
    $$r_{im} = -\left[ \frac{\partial L(y_i, G(x_i))}{\partial }\right]_{G = H_{m-1}};$$
  2. Ajustar um novo preditor fraco $h_m$ aos resíduos $(r_{im},x_i);$
  3. Calcule o multiplicador $\gamma_{m}$ como 
  $$\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n L\left(y_i, H_{m-1}(x_i) + \gamma h_m(x_i)\right);$$
  4. Atualize o modelo:
$$\displaystyle H_{m}(x)=H_{m-1}(x)+\gamma_m h_m(x);$$
* Defina o classificador final como 
  $$G(x) = H_M(x).$$

### Vantagens
* Garante, ao menos na amostra de treino, um desempenho melhor a cada passo.
* Funciona para Regressão e classificação com adaptação na função perda.
* Não requer nenhum tipo de pré-processamento.

### Desvantagens
* Alto custo computacional;
* É possível (embora pouco provável) ocorrer overfitting;
* Não interpretável;

### Tuning {.fullwidth}
Algumas variações do Gradient Boosting incluem:

 * Sortear um subconjunto de tamanho $N' < N$ da amostra de treino para o ajuste do preditor a cada passo.
 * Encolher os preditores por algum valor $\alpha < 1$, isto é, substituir o passo de atualização por 
 $$\displaystyle H_{m}(x)=H_{m-1}(x)+\alpha \gamma_m h_m(x).$$
 * Utilizar diferentes números de nós quando o preditor fraco utlizado é uma Árvore de Regressão e Classificação.
 
### Implementação {.fullwidth}
Algumas das principais implementações do algoritmo de Gradient Boosting em `R` são:

* Pacote `gbm`: Generalized Boosted Regression Models;
* Pacote `xgboost`: Extreme Gradient Boosting;
* Plataforma `h2o`: www.h2o.ai 


# {.col-2}

## Uma aplicação

### Conjunto de dados MNIST

![](mnist.jpg)


### Resultados


* 60000 Imagens 28x28 pixels de dígitos escritos a mão.
* Classificação dos dígitos (10 categorias) utilizando os valores dos 784 pixels como covariáveis.
* Acurácia de 97.33% no conjunto de teste da competição no Kaggle, utilizando Gradient Boosting, com taxa de aprendizado $\alpha = 0.08$, árvores com 7 nós nos classificadores e 600 passos de Boosting, sem nenhum tipo de pré-processamento.
* Implementação em `R` utilizando o framework `h2o` e resultados no Kaggle disponíveis no QR-code.

<img src="frame.png" class="qrcode"/>

# {.col-2}

## Conclusão

* Boosting produz classificadores muito eficiêntes;
* Apesar das variações incluirem diversos parâmetros, a variação deles, exceto para casos específicos, não tem grande efeito na qualidade final das predições; Por outro lado, podem reduzir o custo computacional ou alterar a quantidade necessária de passos até produzir um bom preditor;
* A forma com que o algoritmo é construído causa a impressão de que o overfitting deve ocorrer, mas a quantidade necessária de passos para que ele de fato ocorra é muito grande. A visualização exemplifica como os passos tendem a modificar cada vez menos o modelo.
* Por se tratar de modificações sequênciais de um único preditor, não é necessário guardar um grande número de modelos simutaneamente. Por outro lado, isso torna a paralelização mais complicada.

## Referências

* Friedman, J. H. (2001), ‘Greedy function approximation: a gradient boosting machine’,
Annals of statistics pp. 1189–1232.
* Friedman, J., Hastie, T. & Tibshirani, R. (2001), The elements of statistical learning, Vol. 1,
Springer series in statistics New York, NY, USA:.
* Schapire, R. E. (1990), ‘The strenght of weak learnability’, Machine Learning 5, 197–227.
