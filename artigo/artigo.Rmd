---
title: Boosting

blinded: 0

authors: 

- name: Victor Freguglia Souza
  affiliation: "RA: 137784"
  
- name: Leonardo Uchoa Pedreira
  affiliation: "RA: 156231"

keywords: 
- Gradient Boosting
- Classificação
- Imagens
- Componentes Principais
- Árvores de Classificação

abstract: |
  É apresentada a ideia geral de Boosting e descrita detalhadamente a consrtução do algoritmo de Gradient Boosting e suas principais características. O algoritmo é aplicado a dados de classficação de peças de roupa através de imagens e o seu desempenho é comparado com o das Florestas Aleatórias e versões com pré-processamento utilizando Análise de Componentes Principais nas covariáveis. O Gradient Boosting apresenta resultados superiores aos das Florestas Aleatórias mesmo com um número menor de iterações, bem como custo computacional. A inclusão de PCA diminui o poder preditivo de ambas os algoritmos. Por fim, é feita uma discussão sobre os aspectos teóricos e características do problema que levaram às diferenças.

bibliography: bibliography.bib
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage[portuguese]{algorithm2e}
- \usepackage[portuguese]{babel}
- \usepackage{amssymb}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage[table]{xcolor}
output: rticles::asa_article
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      fig.height = 4) 
```

```{r load_pkgs}
library(tidyverse)
library(caret)
library(doMC)
library(xgboost)
library(knitr)
library(kableExtra)
```

```{r load_mnist, cache = TRUE}
if (file.exists("fashion-mnist_train.csv")){
  treino <- read_csv("fashion-mnist_train.csv", col_names = TRUE)
  teste <- read_csv("fashion-mnist_test.csv", col_names = TRUE)
} else if (file.exists("../fashion-mnist_train.csv")) {
  treino <- read_csv("../fashion-mnist_train.csv", col_names = TRUE)
   teste <- read_csv("../fashion-mnist_test.csv", col_names = TRUE)
} else {
  stop("Não achei o arquivo do arquivo :( Precisa colocar o arquivo 'fashion-mnist_train.csv' na pasta.")
}

names(treino)[1] <- "Y"
names(teste)[1] <- "Y"
treino$Y <- as.factor(treino$Y)
teste$Y <- as.factor(teste$Y)
levels(treino$Y) <- c("T-Shirt", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle Boot")
levels(teste$Y) <- c("T-Shirt", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle Boot")

set.seed(4)
ids_10 <- sample(1:60000, 6000)
out <- 1:60000; out <- out[!out%in%ids_10]
id_valid <- list(valid = ids_10)
id_train <- list(train = out)
tc <- trainControl(method = "cv", verboseIter = TRUE, 
                   indexOut = id_valid, index = id_train)
```

```{r funcoes_vis}
vis <- function(pixels, classe_real = NULL, classe_pred = NULL){
  n_roupas <- dim(pixels)[1]
  px <- unlist(pixels)
  df <- data.frame(value = as.numeric(px))
  posicao <- expand.grid(roupa = 1:n_roupas,x = 1:28, y = 1:28)
  df <- cbind(df, posicao)

  df_labels <- data.frame(roupa = 1:n_roupas)
  if(!is.null(classe_real)){
    classe_real <- classe_real %>% unlist() %>% as.character()
    df_labels$label <- classe_real}
   if(!is.null(classe_pred)){
    classe_pred <- classe_pred %>% unlist() %>% as.character()
    df_labels$pred <- classe_pred}
  
  df <- left_join(df, df_labels)
  
  p <- df %>% ggplot(aes(x=x, y=y, fill = value, group = roupa)) + geom_tile() + 
    scale_fill_gradient(low = "white", high = "black") +
    theme_void() + scale_y_reverse() + guides(fill = FALSE) +
    facet_wrap(~ roupa)
  if(!is.null(classe_real)){
   p <- p + geom_label(aes(x = 14.5, y = 25, label = label), hjust = 0.5)
  }
  if(!is.null(classe_pred)){
    p <- p + geom_label(aes(x = 14.5, y = 2, label = pred), hjust = 0.5, color = "red")
  }
  return(p)
}
```

```{r ajuste_gbm, cache = TRUE, eval = TRUE}
if (file.exists("ajuste_xgb.RData")){
  load("ajuste_xgb.RData")
} else {
registerDoMC(3)
  set.seed(3)
  xgb <- train(Y ~ ., data = treino, method = "xgbTree", trControl = tc, 
               tuneGrid = expand.grid(nrounds = c(2),
                                      max_depth = c(6),
                                      eta = c(.08),
                                      gamma = 0.001,
                                      colsample_bytree = 1,
                                      subsample = 0.8,
                                      min_child_weight = 1))
  save(xgb, file = "ajuste_xgb.RData")
}
```

```{r ajuste_rf, cache = TRUE, eval = TRUE}
if (file.exists("ajuste_rf.RData")){
  load("ajuste_rf.RData")
} else {
registerDoMC(3)
  set.seed(3)
  rf <- train(Y ~ ., data = treino, method = "rf", trControl = tc)
  save(rf, file = "ajuste_rf.RData")
}
```

```{r ajuste_gbm_pca, cache = TRUE, eval = TRUE}
if (file.exists("ajuste_gbm_pca.RData")){
  load("ajuste_gbm_pca.RData")
} else {
registerDoMC(3)
  set.seed(3)
  tc <- trainControl(method = "cv", verboseIter = TRUE, 
                   indexOut = id_valid, index = id_train,
                   preProcOptions = list(thresh = 0.95))
  xgb_pca <- train(Y ~ ., data = treino, method = "xgbTree", trControl = tc, 
               tuneGrid = expand.grid(nrounds = c(500),
                                      max_depth = c(6),
                                      eta = c(.08),
                                      gamma = 0.001,
                                      colsample_bytree = 1,
                                      subsample = 0.8,
                                      min_child_weight = 1),
               preProcess = "pca")
  save(xgb_pca, file = "ajuste_gbm_pca.RData")
}
```

```{r ajuste_rf_pca, cache = TRUE, eval = TRUE}
if (file.exists("ajuste_rf_pca.RData")){
  load("ajuste_rf_pca.RData")
} else {
  registerDoMC(3)
  set.seed(3)
  tc <- trainControl(method = "cv", verboseIter = TRUE, 
                     indexOut = id_valid, index = id_train,
                     preProcOptions = list(thresh = 1))
  rf_pca <- train(Y ~ ., data = treino, method = "rf", trControl = tc, 
                  preProcess = "pca", tuneGrid = data.frame(mtry = 39), ntree = 1000)
  save(rf_pca, file = "ajuste_rf_pca.RData")
}
```



# Introdução

 Boosting é o nome dado a um tipo de algoritmo que, assim como outros métodos em Machine Learning como Bagging e Florestas Aleatórias, busca combinar um grande número de preditores com baixo poder de predição (isto é, um pouco mais eficientes do que a escolha ao acaso) para compor um bom preditor.  O conceito é fundamentado nas ideias apresentadas em @kearns1988 e @schapire1990.
 
 Diferentemente dos outros métodos citados, onde os preditores fracos que serão combinados são criados de maneira independente (e aleatória devido ao processo de bootstrap), no método de Boosting os preditores fracos são criados de maneira a melhorar o desempenho em regiões com altas taxas de erro. Isto é, se pensarmos que cada preditor tem um "voto" na decisão final, o método nos fornece um comitê em que aqueles que tem grande convicção têm mais poder na decisão. Estes de grande convicção, sabem muito sobre uma parte do espaço amostral (não sei se amostral é a melhor palavra).
 
O algoritmo AdaBoost (de \textit{Adaptative Boosting}), apresentado pela primeira vez em @adaboost, é um dos exemplos mais clássicos de algoritmo de boosting para o problema de classificação binária. Nele, a cada passo $m$, um novo classificador $G_m$ é ajustado com base em uma versão ponderada do conjunto de dados original, na qual o peso de cada observação depende do desempenho do classificador anterior: pontos classificados de maneira errada recebem peso maior, e assim, têm uma chance maior de serem corrigidos pelos classificadores ajustados na próxima iteração. O Algoritmo \ref{adaboost_alg} apresenta a descrição completa do AdaBoost. Note que os classificadores $G_m$ a serem usados não precisam ser de nenhum tipo específico e, portanto, se comporta como um parâmetro a ser escolhido por um processo de regulagem (tuning) do problema. Apesar disso, o mais comum, pela grande flexibilidade, é a árvore de classificação e regressão (CART).

\begin{algorithm}[h]
\Inicio{
  $(y_i, x_i), i = 1,\dots, N$; $y_i \in \{-1,1\}$; $x_i \in \mathbb{R}^p$;\\
  Inicie todos pesos $w_i = 1/N$;\\
  \Para{$m = 1,\dots,M$}{
    1. Ajuste um classificador $G_m(x)$;\\
    2. Calcular 
    
    $$\text{err}_m = \frac{\sum_{i = 1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i = 1}^N w_i};$$\\
    3. Calcular $\alpha_m = \log \left(\frac{1 - \text{err}_m}{\text{err}_m}\right)$;\\
    4. Atualizar $w_i \leftarrow w_i \exp \left( \alpha_m I(y_i \neq G_m(x_i)) \right), i = 1, \dots, N$;
  }
  Defina o classificador final como 
  $$G(x) = \text{sign}(\sum_{m = 1}^M \alpha_m I(G_m(x) = k)).$$
}
\caption{Algoritmo AdaBoost apresentado em \cite{elements}. Aqui, as classes do problema de classificação são representadas pelos valores -1 e 1.}
\label{adaboost_alg}
\end{algorithm}

Embora o AdaBoost seja um bom ponto de início para entender o conceito de Boosting, ele foi projetado para resolver problemas de classificação binária, e por isso, não apresenta resultados tão bons quando diretamente adaptado a problemas de classificação múltipla e regressão, então diferentes algoritmos são necessários para esses casos. @SAMME propõe uma modificação do AdaBoost para o caso de classificação múltipla, por exemplo.

Nesse trabalho, por apresentar uma forma mais geral, será considerado o algoritmo Gradient Boosting, que pode lidar com todos os tipos de problemas de predição com a devida escolha da função perda a ser minimizada. O objetivo do trabalho é apresentar e discutir as principais propriedades do Gradient Boosting, bem como possíveis ajustes e comparar com outro método baseado em árvores (Florestas Aleatórias) através de uma aplicação a dados reais. A parte teórica do método e suas descrição detalhada são apresentadas na Seção \ref{sec_metodologia}. O método é então aplicado ao conjunto de dados Fahsion-MNIST, descrito na Seção \ref{sec_dados} e os resultados são mostrados na Seção \ref{sec_apli}.
  
# Metodologia \label{sec_metodologia}
## Contexto e Visão Geral

Em problemas de regressão, escolher o melhor preditor significa estimar uma função $f^*$ que minimiza o risco, definido como o valor esperado da função de perda $L(\cdot,\cdot)$, ou seja,

\begin{equation}
f^* = \arg \min_{f \in \mathcal{F}} \mathbb{E}_{Y|x} \big[ L(Y,f(x)) | x \big ].
\label{eq:general_argmin}
\end{equation}

Note que o risco para um preditor específico $f$ é desconhecido, uma vez que não assumimos nenhuma distribuição, o que impossibilita o cálculo do valor esperado da função perda para esse preditor. Uma boa estratégia para criação da função de predição $f \in \mathcal{F}$ consiste em escolher preditores de forma a minimizar o risco empírico. Isto é,

\begin{equation}
f^* = \arg \min_{f \in \mathcal{F}} \sum_{i=1}^N  L(y_i,f(x_i)) \\
\label{eq:general_emp_argmin}
\end{equation}

Uma forma de resolver o problema \ref{eq:general_emp_argmin} é via utilização de algorítmos numéricos. Gradiente descendente, por exemplo, é uma estratégia habitualmente utilizada, que levaria ao algorítmo

\begin{equation}
f_m = f_{m-1} - \rho_k \sum_{i=1}^N \nabla_f L(y_i,f(x_i))
\label{eq:general_emp_grad_argmin}
\end{equation}

Entretanto, @boosting2001 cita que simplesmente utilizar isto teria efeitos catastróficos no modelo preditivo. Primeiro pois só são levados em consideração as observações que temos (isto é, não existe nenhuma intenção de extrapolar poder preditivo, como a separação entre teste e treinamento ou validação cruzada faria) e segundo pois não é levada em consideração a relação entre as covariáveis. Para contornar este problema, o autor sugere sequencialmente (de forma aditiva) ajustar uma quantidade $M$ de árvores de decisão aos pseudo-resíduos, obtidos de predições subsequentes\label{inline:context_resid&trees}. Assim, o problema de otimização \ref{eq:general_emp_argmin} se torna

\begin{equation}
\begin{aligned}
& f^* = \arg \min_{f \in \mathcal{F}} \sum_{i=1}^N  L(y_i,f(x_i)) \\
& \text{onde } \mathcal{F} \text{ é o conjunto de árvores todas as de decisão},
\end{aligned}
\label{eq:restricted_emp_argmin}
\end{equation}

e que oferece como preditor $f_{boost}(x) = \sum_{m=1}^M \text{Árvore}_m$.

## Árvores de decisão

Árvore de decisão é uma técnica que busca criar partições disjuntas $R_j$, $j=1, \dots, J$ e associar constantes $d_j$ a cada partição (@elements). Neste caso, se uma observação $x$ pertence à região $R_j$, a predição para ela será $d_j$. Formalmente, tal árvore pode ser escrita como

\begin{equation}
T(x;\Theta) = \sum_{j=1}^J d_j I(x \in R_j)
\end{equation}
 
onde $\Theta = \{ R_j,d_j \}$, $j=1, \dots, J$. Com o objetivo de usar várias árvores em \ref{eq:restricted_emp_argmin}, precisamos de uma maneira de construir $\Theta$. Ou seja, na ótica da Teoria de Decisão, para a m-ésima árvore (ou m-ésimo passo) queremos

\begin{equation}
\hat{\Theta}_m = \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x;\Theta)),
\label{eq:tree_region_argmin}
\end{equation}

pois como citado anteriormente (\ref{inline:context_resid&trees}) o ajuste é feito de maneira aditiva e nos resíduos. Para isto existem métodos para definir as regiões $R_j^m$, como o de Particionamento Recursivo [@elements]. Uma vez conhecidas as regiões $R_j^m$ da $m$-ésima árvore, é necessário estimar ainda $d_j^m$  

\begin{equation}
\hat{d}_j^m = \arg \min_{d} \sum_{i: x_i \in R_j^m} L(y_i,f_{m-1}(x_i) + d),
\label{eq:tree_pred_argmin}
\end{equation}

que depende da função de perda escolhida. @elements fornece uma tabela de soluções analíticas para $d$, de acordo com algumas perdas. Assim, conseguimos obter $\Theta_m$, $\forall m=1, \dots, M$.
 
## Otimização por Gradiente 

Gradiente Descendente é um método de otimização numérica que mínimos de uma função. Sua proposta é começar em um ponto inicial e incrementar a função avaliada naquele na direção oposta ao gradiente, em uma certa "velocidade" $\rho_k$, como em \ref{eq:general_emp_grad_argmin}. As iterações são dadas por

\begin{equation}
f_m = f_{m-1} - \rho_k \sum_{i=1}^N \big[ \nabla_f L(y_i,f(x_i)) \big]_{f(x_i) = f_{m-1}(x_i)}
\label{eq:grad_general_problem}
\end{equation}

onde 

\begin{equation}
\rho_k = \arg \min_{\rho} L(f_{m-1}(x_i) - \rho \big[ \nabla_f L(y_i,f(x_i)) \big]_{f(x_i) = f_{m-1}(x_i)}).
\label{eq:grad_step_problem}
\end{equation}

## Gradient Boosting

A direção fornecida da solução pelo método do Gradiente e sua conexão com os resíduos fornecem a intuição e a engrenagem por trás do Boosting. Boosting é baseado em modelos aditivos, onde sequencialmente são ajustam modelos lineares generalizados nos resíduos. Ou seja, no passo $m$, temos o problema

\begin{equation}
\min_{\beta_m,\Theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \beta_m b(x_i;\Theta_m))
\end{equation}

onde $b$ é uma função de base (uma função constante por partes, no caso das árvores). Se considerarmos a perda quadrática, o problema torna-se

\begin{equation}
\min_{\beta_m,\Theta_m} \sum_{i=1}^N (r_{m-1}(x_i) - \beta_m b(x_i;\Theta_m))^2 ,
\end{equation}

em que $r_{m-1}(x_i) = y_i - f_{m-1}(x_i)$. Ao compararmos a equação acima ao problema de regressão linear simples, $r_{m-1}(x_i)$ toma o papel de $y_i$ e $\beta_m b(x_i;\gamma_m)$, o papel de $\beta x_i$. Portanto, a analogia leva à conclusão de se ajustar uma função de base aos resíduos.

<!---Se olharmos a equação equação \ref{eq:tree_region_argmin}, a predição da árvore $T(x_i;\Theta)$ é justamente o argumento que fornece o mínimo. Este é o mesmo papel que o valor negativo do gradiente desempenha em \ref{eq:general_emp_argmin}, o que os tornam similares, em certo sentido. Além disso, também existe a semelhança entre \ref{eq:tree_pred_argmin} e \ref{eq:grad_step_problem} (onde a diferença é que, no caso das árvores, a busca pela solução ótima é restrita ao nó terminal).--->

Para perceber onde tudo se encaixa, vamos voltar ao exemplo da perda quadrática, <!---Na equação \ref{eq:grad_general_problem}, --->

$$
\big[ \nabla_f L(y_i,f(x_i)) \big]_{f(x_i) = f_{m-1}(x_i)} = -2(y_i - f_{m-1}(x_i)) := -g_{im}.
$$

Isto fornece

\begin{align*}
\hat{\Theta}_m &= \arg \min_{\Theta_m} \sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x;\Theta))  \\
 &= \arg \min_{\Theta_m} \sum_{i=1}^N (y_i - f_{m-1}(x_i) - T(x;\Theta))^2 \\
 &= \arg \min_{\Theta_m} \sum_{i=1}^N (-g_{im} - T(x;\Theta))^2.
\end{align*}

De acordo com a analogia anterior para o caso de modelos aditivos, $g_{im}$ aqui tem uma forte conexão com os resíduos (para este exemplo, ele de fato é). Na verdade, ele é chamado de \textit{pseudo-resíduo}, pois para problemas de classificação, os "resíduos" são, na verdade, a margem de classificação (@elements cita como exemplo a perda exponencial e sua interpretação para o algorítmo AdaBoost). Esta é a principal motivação do Boosting, a conexão entre os pseudo-resíduos e a direção do gradiente. 

Se juntarmos as idéias e formularmos um algorítmo, temos o boosting por gradiente (ou Gradient Boosting, ou Gradient Boosting Machine, GBM) como feito em @boosting2001, obtemos o algoritmo a seguir.

 \begin{algorithm}[H]
 \caption{Algoritmo Gradient Boosting apresentado em \cite{boosting2001}.}
\Inicio{
  $(y_i, \textbf{x}_i), i = 1,\dots, N$;\\
  Inicie com o preditor constante $f_0(x) = c^* =  \arg \min_c \sum_{i = 1}^N L(y_i,c)$;\\
  \Para{$m = 1,\dots,M$}{
    1. Calcular
    $$r_{im} = -\left[ \frac{\partial L(y_i, f(x_i))}{\partial f}\right]_{G = f_{m-1}(x_i)};$$\\
    2. Ajustar uma nova árvore $f_m$ ao conjunto de dados $(r_{im},\textbf{x}_i)$ com J regiões terminais $R_j^m, j=1,\dots,J$\\
    3. Para $j=1,\dots,J$, obtenha
    $$
    \hat{d_j^m} = \arg \min_{d} \sum_{i=1}^N L(y_i,f_{m-1}(x_i) + d);
    $$\\
    4. Atualize $f_m (x_i) = f_{m-1} (x_i) + \sum_{j=1}^J \hat d_j^m I(x_i \in R_j^m)$;
  }
  O preditor final é então $f_{boost}(x) = f_M(x)$.
}

\label{gradboost_alg}
\end{algorithm}



**Importante:** Boosting surgiu para problemas de classificação binária e foi adaptado para regressão e classificação de várias categorias. Para classificação de K classes, para $m=1,\dots ,M$, ajuste K árvores de classificação binária e use uma função perda apropriada, como a softmax ( @boosting2001 ), que é a mais habitual para este tipo de problema.

Para ajustar o modelo de Gradient Boosting, algumas variações são feitas para obter maior flexibilidade na prática. Algumas delas incluem: 
 
 - Escolher a profundidade máxima das árvores: Isso permite balancear custo computacional (quanto mais simples, mais rápido o ajuste), interação entre variáveis (quanto mais profunda, mais variáveis são comparadas para se obter a região) e problemas com sobre-ajuste (combinações de árvores muito complexas poderiam causar sobre-ajuste rapidamente pela descida do gradiente, enquanto árvores simples demorariam muito).
 - Taxa de aprendizado ($0<\eta<1$): Ajuda a controlar o sobre-ajuste diminuindo a importância das novas árvores modificando a atualização para $f_m (x_i) = f_{m-1} (x_i) + \eta \sum_{j=1}^J \hat d_j^m I(x_i \in R_j^m)$.
 - Amostrar uma fração dos dados e/ou das covariáveis para ajustar cada nova árvore: Diminui as chances de ocorrer sobre-ajuste

## PCA Whitening

Árvores de decisão buscam fazer divisões do espaço em regiões perpendiculares a algum eixo e a definição dessas regiões é feita de acordo com características da resposta nos pontos, mas não dos pontos em si. Isso faz com que as predições feitas sejam invariantes a transformações lineares de cada variável individualmente. Por exemplo, se a melhor divisão em um determinado nó era com a regra $(x_1 >2)$, os mesmos grupos serão obtidos pelas regra $(2x_1>4)$ ou $(x_1 + 1 > 3)$. Portanto, o ajuste de árvores (e consequentemente do Gradient Boosting) são invariantes por translação e escala das covariáveis individualmente, como normalizações. Logo, esse tipo de pré-processamento não traz nenhum tipo de benefício ou prejuízo para métodos baseados em árvores.

Por outro lado, árvores não são capazes de criar regras como $(x_1 + x_2 > 0)$, embora esse tipo de regra possa ser aproximado por várias regras consecutivas envolvendo apenas uma variável por vez. Isso faz com que sejam consideradas possíveis transformações lineares que combinem variáveis, a fim de se conseguir criar regras que dividem o espaço em outras direções. Se uma regra envolvendo uma combinação linear de covariáveis é ótima, então essa regra única seria mais precisa e mais simples do que uma sequência de regras "univariadas".

Uma das maneiras de se combinar variáveis é por meio de Análise de Componentes Principais ou PCA [@pca], onde se busca uma transformação ortogonal da matriz $X$, que faça uma rotação do espaço na direção de maior variabilidade. As componentes (combinações lineares) dificilmente têm qualquer interpretabilidade, mas podem ser úteis, por exemplo para redução de dimensionalidade, uma vez que a maioria da variabilidade usualmente se concentra em um número de componentes relativamente menor que a dimensão original e são não-correlacionadas.

É de interesse investigar os efeitos do uso desse tipo de rotação para o algoritmo de Gradient Boosting, se a rotação pode tornar as divisões do espaço pelas árvores mais eficiente em termos de predição e/ou computacionais e comparar com outros métodos baseados em árvores, como Florestas Aleatórias.


# Conjunto de Dados Fashion-MNIST \label{sec_dados}

  Para uma ilustração do funcionamento do método de Gradient Boosting com dados reais, será considerado o problema de classificação no conjunto de dados Fashion-MNIST. Similar ao popular conjunto de dados MNIST de classificação de dígitos, o Fashion-MNIST também conta com pequenas imagens de 28 por 28 pixels em escala de cinza para classificação de peças de roupa em 10 categorias. São elas:
  
  - T-Shirt

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "T-Shirt")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Trouser

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Trouser")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Pullover

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Pullover")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Dress

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Dress")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Coat

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Coat")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Shirt

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Shirt")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Sandal

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Sandal")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Sneaker

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Sneaker")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Bag

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Bag")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 - Ankle Boot

```{r, fig.height = 0.5, fig.width=6, cache = TRUE}
idx <- which(treino$Y == "Ankle Boot")[1:10]
vis(treino[idx,-1]) + facet_grid(cols = vars(roupa))
```

 Estão disponíveis em https://www.kaggle.com/zalando-research/fashionmnist/home dois conjuntos de dados: Um conjunto de treinamento, contendo 60000 imagens e um conjunto de teste, com 10000 imagens. Tanto o conjunto de treino quanto o de teste estão balanceados com exatamente 10% (6000 e 1000, respecitavamente) observações de cada categoria. Cada pixel da imagem é considerado uma variável preditora, totalizando $28 \times 28 = 784$. Portanto, temos um total de $n = 60000$ observações e $p = 784$. 
 
 A priori, parece existir um grande potencial de confundimento entre algumas categorias. Por exemplo, \textit{Coat, Shirt, Dress} e \textit{T-Shirt} apresentam estruturas similares, com estruturas parecidas para o tronco e as mangas. As diferenças ficam em alguns detalhes, como as diferença de comprimento para a manga e a parte do tronco. A mesma dificuldade também parece aparecer na comparação entre \textit{Sneaker} e \textit{Ankle Boot}, onde a principal diferença é apenas no comprimento da parte do tornozelo. Outra coisa que parece diferencias batante imagens da mesma categoria é o ângulo da foto. Embora a maioria tenha sido tirada de frente para a peça, algumas estão em posições diferentes, podendo gerar um confundimento extra.

# Aplicação \label{sec_apli}

Existem diversas ferramentas disponíveis para ajuste de algoritmos de Boosting. Em particular, para a linguagem R, os mais populares são os pacotes \textit{"xgboost" (Extreme Gradient Boosting), "lightGBM"} e no framework "h2o". Para esse trabalho, os ajustes serão feitos usando o pacote \textit{"xgboost"}, por apresentar maior flexibilização dos parâmetros de maneira mais simples e por também estar implementado através da função \textit{train} do pacote \textit{caret}, o que facilita a comparação com outros modelos ajustados.

O conjunto original de treinamento foi separado em dois subconjuntos, um subconjunto para realizar o treinamento de fato (90%) e um outro subconjunto de validação (10%), de maneira aleatória. O conjunto de teste será deixado exclusivamente para a avaliação da performance dos modelos ajustados. Estratégias mais sofisticadas de validação cruzada não foram utilizadas por questões de tempo. Os métodos utilizados tendem a ter ajustes lentos, o que torna o ajuste em um grid de parâmetros muito caro, principalmente se quantidades de árvores $M$ grandes pertencem ao grid.

Os principais objetivos são: Avaliar a qualidade de predição do Gradient Boosting no problema de classificação das peças de roupa, comparar o poder de predição do Boosting com o das Florestas Aleatórias (árvores dependentes versus árvores independentes) e investigar o efeito de pré-processamento para os métodos, por exemplo, através de Análise de Componentes Principais (PCA) nas covariáveis.

Para comparação dos métodos, foram feitos ajustes de diversos tipos de modelos com diferentes configurações de parâmetros. Para resumir as informações, serão reportadas as métricas apenas das configurações de parâmetros que apresentaram melhor desempenho no conjunto de validação, ou seja, do grid escolhido para ajustar cada modelo, apenas a melhor configuração no grid será reportada. Os modelos ajustados para comparação foram:

 - Gradient Boosting com árvores de classificação: Taxa de aprendizado $\eta = 0.08$, árvores $f_m$ com profundindade máxima $4$, taxa de amostragem de 80% para cada árvore e um total de $M = 500$ passos.
  - Florestas Aleatórias: Total de 1000 árvores, com $m = 34 \sim \sqrt{p}$ covariáveis amostradas para cada árvore. 
  - Gradient Boosting com árvores de classificação + PCA Whitening: PCA utilizada no pré-processamento, todas as componentes foram utlizadas. Taxa de aprendizado $\eta = 0.08$, árvores $f_m$ com profundindade máxima $4$, taxa de amostragem de 80% para cada árvore e um total de $M = 500$ passos.
  - Florestas Aleatórias + PCA Whitening: PCA utilizada no pré-processamento, todas as componentes foram utlizadas. Total de 1000 árvores, com $m = 34 \sim \sqrt{p}$ covariáveis amostradas para cada árvore.
  
Os resultados obtidos por cada método em termos de acurácia são apresentados na Tabela \ref{tab:resultados}. Além disso as Tabelas \ref{tab:confusion_gbm} a \ref{confusion_rf_pca} apresentam as matrizes de confusão de cada caso.

```{r resultados, cache = TRUE}
xgb_treino_ac <- mean(predict(xgb,treino) == treino$Y)
xgb_teste_ac <- mean(predict(xgb,teste) == teste$Y)

rf_treino_ac <- mean(predict(rf,treino) == treino$Y)
rf_teste_ac <- mean(predict(rf,teste) == teste$Y)

xgb_pca_treino_ac <- mean(predict(xgb_pca,treino) == treino$Y)
xgb_pca_teste_ac <- mean(predict(xgb_pca,teste) == teste$Y)

rf_pca_treino_ac <- mean(predict(rf_pca,treino) == treino$Y)
rf_pca_teste_ac <- mean(predict(rf_pca,teste) == teste$Y)

df <- data.frame(Treinamento = c(xgb_treino_ac, rf_treino_ac, xgb_pca_treino_ac, rf_pca_treino_ac),
                 Teste = c(xgb_teste_ac, rf_teste_ac, xgb_pca_teste_ac, rf_pca_teste_ac),
                 "Tempo (em segundos)" = c(6935,31794,9794,14559))
rownames(df) <- c("GBM", "Florestas Aleatórias", "GBM + PCA", "Florestas Aleatórias + PCA")

df %>% kable(format = "latex", caption = "Acurácia de cada modelo por conjunto", booktabs = T) %>%
  kable_styling(full_width = T, latex_options = "striped")
```

 
 
```{r confusion_gbm, cache = TRUE}
kable(table(Predito = predict(xgb, teste), Classe = teste$Y), format = "latex",
      caption = "Matriz de Confusão para predições com Gradient Boosting no conjunto de teste", booktabs = T) %>%
    kable_styling(full_width = T, latex_options = c("HOLD_position","striped"), font_size = 8) %>%
  row_spec(0, angle = -45) %>% add_header_above(header = c("", "Predito" = 10))
```
 
 
```{r confusion_rf, cache = TRUE}
kable(table(Predito = predict(rf, teste), Classe = teste$Y), format = "latex",
      caption = "Matriz de Confusão para predições com Florestas Aleatórias no conjunto de teste", booktabs = T) %>%
    kable_styling(full_width = T, latex_options = c("HOLD_position","striped"), font_size = 8) %>%
  row_spec(0, angle = -45) %>% add_header_above(header = c("", "Predito" = 10))
```
 
```{r confusion_xgb_pca, cache = TRUE}
kable(table(Predito = predict(xgb_pca, teste), Classe = teste$Y), format = "latex",
      caption = "Matriz de Confusão para predições com Gradient Boosting com PCA no conjunto de teste", booktabs = T) %>%
    kable_styling(full_width = T, latex_options = c("HOLD_position","striped"), font_size = 8) %>%
  row_spec(0, angle = -45) %>% add_header_above(header = c("", "Predito" = 10))
```

 
```{r confusion_rf_pca, cache = TRUE}
kable(table(Predito = predict(rf_pca, teste), Classe = teste$Y), format = "latex",
      caption = "Matriz de Confusão para predições com Gradient Boosting", booktabs = T) %>%
    kable_styling(full_width = T, latex_options = c("HOLD_position","striped"), font_size = 8) %>%
  row_spec(0, angle = -45) %>% add_header_above(header = c("", "Predito" = 10))
```

# Discussão

Tanto o algoritmo de Gradient Boosting quanto as Florestas Aleatórias apresentaram um excelente desempenho para a classificação das peças de roupa no conjunto de teste, com uma vantagem de pouco mais de 2% de acurácia para o GBM. Além disso, como esperado, a maioria dos casos de confundimento ocorreram entre as classes \textit{Coat, Shirt, Dress} e \textit{T-Shirt}. A Figura \ref{fig:misses} apresenta algumas das peças classificadas de forma errada pelo método de Gradient Boosting. 

```{r misses, cache = TRUE, fig.cap="Exemplos de erros de classificação do GBM. Classe predita em vermelho, classe verdadeira em preto."}
preds <- predict(xgb,teste) 
idx_miss <- which(preds != teste$Y)[1:16]
vis(teste[idx_miss,-1],teste$Y[idx_miss] ,preds[idx_miss])
```

Sobre  o custo computacional, comparado aqui através do tempo de execução (foram utilizadas máquinas do www.kaggle.com para os ajustes), os algoritmos de Florestas Aleatórias demoraram mais que os de Boosting nos 2 casos (com e sem PCA), mesmo que, ao adicionar PCA, o tempo de execução das Florestas Aleatórias tenha reduzido bastante, uma vez que, com menos variáveis importantes, a complexidade das árvores ajustadas tende a diminuir, acelerando o ajuste. Também é importante ressaltar, que se dividirmos o tempo de execução pelo número de árvores ajustadas, no caso com PCA, as árvores ficam mais rápidas que o Boosting pelo menos motivo.

O número de passos $M = 500$ selecionado foi o maior testado no grid de escolha dos parâmetros, o que indica que, possivelmente, a classificação poderia ser ainda mais precisa se fossem utilizados mais passos, enquanto com as 1000 árvores consideradas as Florestas Aleatórias já atingiam 100% de acurácia no conjunto de treinamento, indicando pouco potencial de melhora com o investimento de mais árvores.

Com respeito ao efeito do pré-processamento por Análise de Componentes Principais (PCA), utilizando o método apenas como uma rotação (PCA-Whitening), a consequência foi uma diminuição muito grande no poder de predição dos métodos. Isso acontece porque a rotação apenas faz com que variáveis se transformem na direção de maior variabilidade, mas sem levar em consideração sua relação com a resposta. Como consequência, alguns detalhes importantes para predição mas com pouca variabilidade são "diluídos" em muitas componentes de pouca importância e acabam sendo de difícil identificação pelos preditores.  Esse problema não ocorreria, por exemplo, se regiões lineares discriminassem bem as respotas, o que geralmente não é caso de dados de imagens.

  
# Conclusão

Através da investigação dos resultados da aplicação e da discussão dos pontos apontados nas seções de Discussão e Metodologia, pudemos concluir que o algoritmo de Gradient Boosting produz preditores muito bons por ter sido construído com uma estratégia muito eficiente e que pode ser regulada através da inclusão de diversos parâmetros de ajuste em suas variações para evitar problemas característicos dos problemas. Como consequência, percebemos que, se construído e ajustado de maneira apropriada, ele produz resultados melhores do que a estratégia de Florestas Aleatórias, tanto do ponto de vista de erro de predição quanto em custo computacional. De maneira geral, criar novas árvores simples baseado nas qualidades e defeitos do "comitê" a cada passo parece mais eficiente do que criar novas árvores de maneira aleatória e independente das qualidades e defeitos do preditor.

A rotação da matriz das covariáveis através do uso de Componentes Principais não é apropriada para o tipo de problema considerado, pois a rotação induz a perda de características importantes para o problema de classificação. O fato dos métodos testados serem capazes de criar regiões de separação altamente não-lineares é uma das maiores vantagens de se usar árvores, mas essa vantagem acaba sendo perdida devido à rotação feita. Por outro lado, caso essa rotação fosse apropriada, uma redução de variáveis poderia ser feita para ajustar os modelos de maneira mais rápida, assim, mais passos do Gradient Boosting poderiam ser feitos com o mesmo tempo, possivelmente obtendo preditores ainda melhores.