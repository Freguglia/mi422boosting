---
title: Boosting

blinded: 0

authors: 
- name: Leonardo Uchoa Pedreira
  affiliation: "RA: 156231"
  
- name: Victor Freguglia Souza
  affiliation: "RA: 137784"

keywords: 
- Quando for a versão final.
- Tirar esse campo na .tex;

abstract: |
  The text of your abstract.  200 or fewer words.

bibliography: bibliography.bib
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage[portuguese]{algorithm2e}
- \usepackage[portuguese]{babel}
output: rticles::asa_article
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
# Possivelmente ajustar as figuras aqui direto.
```

```{r load_pkgs}
library(tidyverse)
library(caret)
library(doMC)
```

```{r load_mnist, cache = TRUE}
if (file.exists("mnist_train.csv")){
  MNIST <- read_csv("mnist_train.csv", col_names = FALSE)
} else if (file.exists("../mnist_train.csv")) {
  MNIST <- read_csv("../mnist_train.csv", col_names = FALSE)
} else {
  stop("Não achei o arquivo do arquivo :( Precisa colocar o arquivo que a gente usou pros labs 'mnist_train.csv' na pasta.")
}

names(MNIST)[1] <- "Y"
MNIST$Y <- as.factor(MNIST$Y)

set.seed(1)
idx_teste <- sample(1:nrow(MNIST), 55000, replace = FALSE)
treino <- MNIST[-idx_teste, ]
teste <- MNIST[idx_teste, ]
```

```{r ajuste_gbm, include = FALSE}
tc <- trainControl(verboseIter = TRUE, number = 1, repeats = 1)
gbm <- train(Y ~ ., data = treino, method = "gbm", trControl = tc)
```


# Introdução

- História
 @schapire1990 ;
- Como funciona?
- Pq funciona?
  
# Metodologia
  
  - Ideia, propriedades, etc.

## AdaBoost

O algoritmo AdaBoost (de \textit{Adaptative Boosting}), apresentado pela primeira vez em @adaboost, .....

\begin{algorithm}
\Inicio{
  Inicie todos pesos $w_i = 1/N$;\\
  \Para{$m = 1,\dots,M$}{
    1. Ajuste um classificador $G_m(x)$;\\
    2. Calcular 
    
    $$\text{err}_m = \frac{\sum_{i = 1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i = 1}^N w_i};$$\\
    3. Calcular $\alpha_m = \log \left(\frac{1 - \text{err}_m}{\text{err}_m}\right)$;\\
    4. Atualizar $w_i \leftarrow w_i \exp \left( \alpha_m I(y_i \neq G_m(x_i)) \right), i = 1, \dots, N$;
  }
  Defina o classificador final como 
  $$G(x) = \arg \max_k \sum_{m = 1}^M \alpha_m I(G_m(x) = k).$$
}
\caption{Algoritmo AdaBoost apresentado em \cite{elements} com uma generalização para o caso de múltiplas classes.}
\label{adaboost_alg}
\end{algorithm}

## Tree Boosting

- Gradient Boosting

@schapire1990 

[@elements]

## PCA Whitening

# Dados

Conjunto de dados MNIST.

- Mostrar alguns números

# Aplicação

- Ajuste
- Comparar com Random Forest, nnet, pca-whitened

# Discussão

- Não sei o que entra em discussão/conclusão?
  
# Conclusão